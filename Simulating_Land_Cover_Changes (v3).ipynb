{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd1503b",
   "metadata": {},
   "source": [
    "\n",
    "# Simulating Land Cover Changes Using Diffusion-Based Generative Models\n",
    "\n",
    "### Introduction\n",
    "This notebook implements a diffusion-based generative model to simulate land cover changes using the EuroSAT dataset. The goal is to generate synthetic satellite imagery that reflects specific land cover types, providing insights for urban planning, environmental conservation, and sustainable resource management.\n",
    "\n",
    "LINK TO THE PUBLIC DATASET: https://zenodo.org/records/7711810#.ZAm3k-zMKEA\n",
    "\n",
    "#### Objectives\n",
    "- Analyze and preprocess the EuroSAT dataset.\n",
    "- Implement and train a diffusion-based generative model.\n",
    "- Evaluate the quality of synthetic images using perceptual metrics and classification models.\n",
    "- Visualize land cover changes for targeted simulations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8248a1b8",
   "metadata": {},
   "source": [
    "\n",
    "## Environment Setup\n",
    "\n",
    "### Objectives\n",
    "- Ensure the necessary libraries are installed.\n",
    "- Validate that the computational environment supports MPS (Metal Performance Shaders) for macOS or fall back to CPU.\n",
    "\n",
    "### Implementation\n",
    "The following code checks for required dependencies and device compatibility:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ec412b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.11/site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: rasterio in /opt/anaconda3/lib/python3.11/site-packages (1.4.2)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.11/site-packages (3.8.0)\n",
      "Requirement already satisfied: scikit-image in /opt/anaconda3/lib/python3.11/site-packages (0.22.0)\n",
      "Requirement already satisfied: affine in /opt/anaconda3/lib/python3.11/site-packages (from rasterio) (2.4.0)\n",
      "Requirement already satisfied: attrs in /opt/anaconda3/lib/python3.11/site-packages (from rasterio) (23.1.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.11/site-packages (from rasterio) (2024.8.30)\n",
      "Requirement already satisfied: click>=4.0 in /opt/anaconda3/lib/python3.11/site-packages (from rasterio) (8.1.7)\n",
      "Requirement already satisfied: cligj>=0.5 in /opt/anaconda3/lib/python3.11/site-packages (from rasterio) (0.7.2)\n",
      "Requirement already satisfied: numpy>=1.24 in /opt/anaconda3/lib/python3.11/site-packages (from rasterio) (1.26.4)\n",
      "Requirement already satisfied: click-plugins in /opt/anaconda3/lib/python3.11/site-packages (from rasterio) (1.1.1)\n",
      "Requirement already satisfied: pyparsing in /opt/anaconda3/lib/python3.11/site-packages (from rasterio) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.8 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-image) (1.11.4)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-image) (3.1)\n",
      "Requirement already satisfied: imageio>=2.27 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-image) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-image) (2023.4.12)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-image) (0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "PyTorch Version: 2.5.1\n",
      "MPS Available: True\n",
      "Using Device: MPS\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install rasterio matplotlib scikit-image\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "print(\"Using Device:\", \"MPS\" if torch.backends.mps.is_available() else \"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba97e7c",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset Analysis\n",
    "\n",
    "### Overview\n",
    "The EuroSAT dataset contains labeled satellite imagery of various land cover types, including residential, forest, and farmland. Each image has multiple spectral bands, making it suitable for our generative modeling task.\n",
    "\n",
    "### Goals\n",
    "1. Load and explore the dataset.\n",
    "2. Analyze image properties such as dimensions, spectral bands, and distribution across classes.\n",
    "3. Visualize sample images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d5cf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing folder: Forest\n",
      "Analyzing folder: River\n",
      "Analyzing folder: Highway\n",
      "Analyzing folder: AnnualCrop\n",
      "Analyzing folder: SeaLake\n",
      "Analyzing folder: HerbaceousVegetation\n",
      "Analyzing folder: Industrial\n",
      "Analyzing folder: Residential\n",
      "Analyzing folder: PermanentCrop\n",
      "Analyzing folder: Pasture\n",
      "\n",
      "Category: Forest\n",
      "Image Count: 3000\n",
      "Sample Metadata:\n",
      "{'file_name': 'Forest_2230.tif', 'width': 64, 'height': 64, 'count': 13, 'dtype': 'uint16', 'crs': 'EPSG:32634', 'transform': Affine(9.99609868365168, 0.0, 669210.3057983121,\n",
      "       0.0, -9.985523545152093, 6082324.332088696)}\n",
      "\n",
      "Category: River\n",
      "Image Count: 2500\n",
      "Sample Metadata:\n",
      "{'file_name': 'River_1817.tif', 'width': 64, 'height': 64, 'count': 13, 'dtype': 'uint16', 'crs': 'EPSG:32631', 'transform': Affine(9.989362907895012, 0.0, 641130.4851580395,\n",
      "       0.0, -10.005525463576804, 5056360.7423)}\n",
      "\n",
      "Category: Highway\n",
      "Image Count: 2500\n",
      "Sample Metadata:\n",
      "{'file_name': 'Highway_1320.tif', 'width': 64, 'height': 64, 'count': 13, 'dtype': 'uint16', 'crs': 'EPSG:32632', 'transform': Affine(9.99865300990157, 0.0, 568430.884031,\n",
      "       0.0, -10.053586438349257, 5777403.5593)}\n",
      "\n",
      "Category: AnnualCrop\n",
      "Image Count: 3000\n",
      "Sample Metadata:\n",
      "{'file_name': 'AnnualCrop_1855.tif', 'width': 64, 'height': 64, 'count': 13, 'dtype': 'uint16', 'crs': 'EPSG:32633', 'transform': Affine(10.001884564999976, 0.0, 754979.756583425,\n",
      "       0.0, -10.003029416720862, 5210917.079655392)}\n",
      "\n",
      "Category: SeaLake\n",
      "Image Count: 3000\n",
      "Sample Metadata:\n",
      "{'file_name': 'SeaLake_1126.tif', 'width': 64, 'height': 64, 'count': 13, 'dtype': 'uint16', 'crs': 'EPSG:32631', 'transform': Affine(10.00175214225205, 0.0, 592468.6809028842,\n",
      "       0.0, -10.001435894077296, 5730737.038535239)}\n",
      "\n",
      "Category: HerbaceousVegetation\n",
      "Image Count: 3000\n",
      "Sample Metadata:\n",
      "{'file_name': 'HerbaceousVegetation_921.tif', 'width': 64, 'height': 64, 'count': 13, 'dtype': 'uint16', 'crs': 'EPSG:32634', 'transform': Affine(10.005071487058885, 0.0, 753649.9375293177,\n",
      "       0.0, -10.007740365079648, 4215802.848763889)}\n",
      "\n",
      "Category: Industrial\n",
      "Image Count: 2500\n",
      "Sample Metadata:\n",
      "{'file_name': 'Industrial_216.tif', 'width': 64, 'height': 64, 'count': 13, 'dtype': 'uint16', 'crs': 'EPSG:32632', 'transform': Affine(10.019880164706368, 0.0, 571794.850351,\n",
      "       0.0, -9.950182162162319, 6372389.69146)}\n",
      "\n",
      "Category: Residential\n",
      "Image Count: 3000\n",
      "Sample Metadata:\n",
      "{'file_name': 'Residential_1535.tif', 'width': 64, 'height': 64, 'count': 13, 'dtype': 'uint16', 'crs': 'EPSG:32632', 'transform': Affine(9.949611062499752, 0.0, 688487.21763,\n",
      "       0.0, -10.002597000007517, 5387171.85895)}\n",
      "\n",
      "Category: PermanentCrop\n",
      "Image Count: 2500\n",
      "Sample Metadata:\n",
      "{'file_name': 'PermanentCrop_1456.tif', 'width': 64, 'height': 64, 'count': 13, 'dtype': 'uint16', 'crs': 'EPSG:32630', 'transform': Affine(10.000340401337686, 0.0, 266982.11343617394,\n",
      "       0.0, -10.009015411765807, 4529921.73424647)}\n",
      "\n",
      "Category: Pasture\n",
      "Image Count: 2000\n",
      "Sample Metadata:\n",
      "{'file_name': 'Pasture_789.tif', 'width': 64, 'height': 64, 'count': 13, 'dtype': 'uint16', 'crs': 'EPSG:32630', 'transform': Affine(9.981066330232615, 0.0, 633908.8290869303,\n",
      "       0.0, -10.01078331896409, 5819438.281734267)}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the dataset\n",
    "DATASET_PATH = \"EuroSAT_MS\"\n",
    "\n",
    "# Function to read a single image and return its metadata\n",
    "def read_image_metadata(image_path):\n",
    "    try:\n",
    "        with rasterio.open(image_path) as src:\n",
    "            return {\n",
    "                \"file_name\": os.path.basename(image_path),\n",
    "                \"width\": src.width,\n",
    "                \"height\": src.height,\n",
    "                \"count\": src.count,  \n",
    "                \"dtype\": src.dtypes[0], \n",
    "                \"crs\": str(src.crs), \n",
    "                \"transform\": src.transform\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to analyze all images in a folder\n",
    "def analyze_folder(folder_path):\n",
    "    metadata_list = []\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.endswith('.tif')]\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(lambda f: read_image_metadata(os.path.join(folder_path, f)), image_files)\n",
    "        metadata_list.extend([res for res in results if res is not None])\n",
    "    return metadata_list\n",
    "\n",
    "# Main function to analyze the dataset\n",
    "def analyze_dataset(dataset_path):\n",
    "    dataset_summary = {}\n",
    "    for folder_name in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(f\"Analyzing folder: {folder_name}\")\n",
    "            metadata_list = analyze_folder(folder_path)\n",
    "            dataset_summary[folder_name] = {\n",
    "                \"image_count\": len(metadata_list),\n",
    "                \"metadata\": metadata_list\n",
    "            }\n",
    "    return dataset_summary\n",
    "\n",
    "# Analyze dataset\n",
    "dataset_summary = analyze_dataset(DATASET_PATH)\n",
    "\n",
    "# Print summary\n",
    "for category, summary in dataset_summary.items():\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    print(f\"Image Count: {summary['image_count']}\")\n",
    "    print(\"Sample Metadata:\")\n",
    "    if summary['metadata']:\n",
    "        print(summary['metadata'][0])  # Print metadata of the first image\n",
    "\n",
    "image_sizes = [\n",
    "    (meta['width'], meta['height'])\n",
    "    for category in dataset_summary.values()\n",
    "    for meta in category['metadata']\n",
    "]\n",
    "size_counts = Counter(image_sizes)\n",
    "sizes, counts = zip(*size_counts.items())\n",
    "plt.bar([f\"{w}x{h}\" for w, h in sizes], counts)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Image Size Distribution\")\n",
    "plt.xlabel(\"Dimensions (Width x Height)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f9e30f",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "### Steps\n",
    "1. Normalize pixel values for all images to the range [0, 1].\n",
    "2. Split the dataset into training, validation, and testing sets.\n",
    "3. Ensure the splits maintain class balance.\n",
    "4. Prepare DataLoaders for model training.\n",
    "\n",
    "### Implementation\n",
    "The following code performs these preprocessing steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017cea60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category-to-Label Mapping: {'Forest': 0, 'River': 1, '.DS_Store': 2, 'Highway': 3, 'AnnualCrop': 4, 'SeaLake': 5, 'HerbaceousVegetation': 6, 'Industrial': 7, 'Residential': 8, 'PermanentCrop': 9, 'Pasture': 10}\n",
      "Processing category: Forest\n",
      "Processing category: River\n",
      "Processing category: Highway\n",
      "Processing category: AnnualCrop\n",
      "Processing category: SeaLake\n",
      "Processing category: HerbaceousVegetation\n",
      "Processing category: Industrial\n",
      "Processing category: Residential\n",
      "Processing category: PermanentCrop\n",
      "Processing category: Pasture\n",
      "Training set size: 18900\n",
      "Validation set size: 2700\n",
      "Test set size: 5400\n",
      "Category Mapping: {'Forest': 0, 'River': 1, '.DS_Store': 2, 'Highway': 3, 'AnnualCrop': 4, 'SeaLake': 5, 'HerbaceousVegetation': 6, 'Industrial': 7, 'Residential': 8, 'PermanentCrop': 9, 'Pasture': 10}\n",
      "First training sample shape: (13, 64, 64), Label: 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def normalize_image(image_path):\n",
    "    with rasterio.open(image_path) as src:\n",
    "        data = src.read()  \n",
    "        normalized_data = data / 65535.0 \n",
    "        return normalized_data\n",
    "\n",
    "# Function to prepare the combined dataset\n",
    "def prepare_combined_dataset(dataset_path, test_size=0.2, val_size=0.1):\n",
    "    data = []\n",
    "    labels = []\n",
    "    categories = os.listdir(dataset_path)\n",
    "    \n",
    "    # Assign numeric labels to categories\n",
    "    label_map = {category: idx for idx, category in enumerate(categories)}\n",
    "    print(f\"Category-to-Label Mapping: {label_map}\")\n",
    "    \n",
    "    # Load all data and labels\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(dataset_path, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            print(f\"Processing category: {category}\")\n",
    "            for file_name in os.listdir(category_path):\n",
    "                if file_name.endswith('.tif'):\n",
    "                    image_path = os.path.join(category_path, file_name)\n",
    "                    image_data = normalize_image(image_path)\n",
    "                    data.append(image_data)\n",
    "                    labels.append(label_map[category])\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Split the dataset into train, validation, and test sets\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        data, labels, test_size=test_size, stratify=labels, random_state=42\n",
    "    )\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        train_data, train_labels, test_size=val_size / (1 - test_size), stratify=train_labels, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Print dataset sizes\n",
    "    print(f\"Training set size: {len(train_data)}\")\n",
    "    print(f\"Validation set size: {len(val_data)}\")\n",
    "    print(f\"Test set size: {len(test_data)}\")\n",
    "    \n",
    "    return (train_data, train_labels), (val_data, val_labels), (test_data, test_labels), label_map\n",
    "\n",
    "# Prepare the dataset\n",
    "(train_data, train_labels), (val_data, val_labels), (test_data, test_labels), label_map = prepare_combined_dataset(DATASET_PATH)\n",
    "\n",
    "# Access a training sample\n",
    "print(f\"Category Mapping: {label_map}\")\n",
    "print(f\"First training sample shape: {train_data[0].shape}, Label: {train_labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9345f0",
   "metadata": {},
   "source": [
    "\n",
    "## Diffusion-Based Generative Model\n",
    "\n",
    "### Model Architecture\n",
    "We implement a UNet architecture for the diffusion-based generative model. The model consists of:\n",
    "1. An encoder for downsampling the input image.\n",
    "2. A decoder for reconstructing the image from the latent representation.\n",
    "\n",
    "### Reverse Diffusion Process\n",
    "The reverse diffusion process generates synthetic images by denoising random noise iteratively.\n",
    "\n",
    "### Implementation\n",
    "The following code defines the model and diffusion process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdf12f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.models import vgg16\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 5e-4\n",
    "EPOCHS = 500\n",
    "#EPOCHS = 10\n",
    "IMG_SIZE = 64\n",
    "NUM_CHANNELS = 13\n",
    "TIMESTEPS = 1000\n",
    "\n",
    "# Define noise schedule\n",
    "def noise_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    steps = torch.linspace(0, timesteps, timesteps)\n",
    "    betas = (1 - torch.cos((steps / timesteps) * torch.pi)) / 2 * (beta_end - beta_start) + beta_start\n",
    "    return betas.clamp(min=1e-8)\n",
    "\n",
    "# Forward diffusion\n",
    "def forward_diffusion(x_0, t, betas):\n",
    "    noise = torch.randn_like(x_0)\n",
    "    batch_size = x_0.shape[0]\n",
    "    sqrt_alpha_t = torch.sqrt(1.0 - betas.cumsum(dim=0))[t].view(batch_size, 1, 1, 1)\n",
    "    sqrt_betas = torch.sqrt(betas[t]).view(batch_size, 1, 1, 1)\n",
    "    return sqrt_alpha_t * x_0 + sqrt_betas * noise, noise\n",
    "\n",
    "def reverse_diffusion(model, timesteps, betas, shape, device):\n",
    "    images = torch.randn(shape, device=device) \n",
    "    for t in range(timesteps - 1, -1, -1):\n",
    "        noise_pred = model(images)\n",
    "        beta_t = betas[t]\n",
    "        images = (images - beta_t * noise_pred) / (1 - beta_t).sqrt()\n",
    "    return images\n",
    "\n",
    "\n",
    "# Denoising model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_dim=32):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_dim, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hidden_dim, hidden_dim, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, out_channels, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        return self.decoder(latent)\n",
    "\n",
    "# Pretrained VGG model for perceptual loss\n",
    "vgg_model = vgg16(pretrained=True).features[:5].to(\"mps\")  # Move to MPS\n",
    "vgg_model.eval()\n",
    "for param in vgg_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Custom normalization for VGG\n",
    "imagenet_mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "imagenet_std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "\n",
    "def normalize_for_vgg(image):\n",
    "    device = image.device \n",
    "    mean = imagenet_mean.to(device)\n",
    "    std = imagenet_std.to(device)\n",
    "    return (image - mean) / std\n",
    "\n",
    "def reduce_channels(image):\n",
    "    return torch.stack([image[:, 0, :, :], image[:, 6, :, :], image[:, 12, :, :]], dim=1)\n",
    "\n",
    "def perceptual_loss(predicted, target):\n",
    "    predicted = normalize_for_vgg(reduce_channels(predicted))\n",
    "    target = normalize_for_vgg(reduce_channels(target))\n",
    "    predicted_features = vgg_model(predicted)\n",
    "    target_features = vgg_model(target)\n",
    "    return ((predicted_features - target_features) ** 2).mean()\n",
    "\n",
    "# Loss function\n",
    "def denoising_loss(model, x_0, betas, timesteps):\n",
    "    t = torch.randint(0, timesteps, (x_0.size(0),), device=x_0.device).long()\n",
    "    x_t, noise = forward_diffusion(x_0, t, betas)\n",
    "    predicted_noise = model(x_t)\n",
    "    mse_loss = ((noise - predicted_noise) ** 2).mean()\n",
    "    perceptual_component = perceptual_loss(predicted_noise, noise)\n",
    "    return mse_loss + 0.1 * perceptual_component\n",
    "\n",
    "# Load dataset\n",
    "def load_dataset(data, batch_size):\n",
    "    tensor_data = torch.tensor(data, dtype=torch.float32)\n",
    "    dataset = TensorDataset(tensor_data)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train model\n",
    "def train_model(model, train_dataloader, val_dataloader, optimizer, betas, timesteps, device):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    early_stop_count = 0\n",
    "    patience = 10\n",
    "    loss_history, val_loss_history = [], []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (x,) in enumerate(train_dataloader):\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = denoising_loss(model, x, betas, timesteps)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        loss_history.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_x, in val_dataloader:\n",
    "                val_x = val_x.to(device)\n",
    "                val_loss += denoising_loss(model, val_x, betas, timesteps).item()\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"best_diffusion_model.pth\")\n",
    "            print(\"Saved Best Model\")\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stop_count >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Plot loss\n",
    "    plt.plot(loss_history, label=\"Train Loss\")\n",
    "    plt.plot(val_loss_history, label=\"Val Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b53a984",
   "metadata": {},
   "source": [
    "\n",
    "## Model Training\n",
    "\n",
    "### Objectives\n",
    "- Train the diffusion model to generate high-quality synthetic images.\n",
    "- Monitor training and validation losses for convergence.\n",
    "\n",
    "### Training Parameters\n",
    "- **Batch Size:** 16\n",
    "- **Learning Rate:** 5e-4\n",
    "- **Epochs:** 500    Actual count of epochs to be used \n",
    "- **Timesteps:** 1000\n",
    "\n",
    "### Implementation\n",
    "The following code trains the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9331cabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (18900, 13, 64, 64)\n",
      "Validation data shape: (2700, 13, 64, 64)\n",
      "Epoch 1/10, Train Loss: nan, Val Loss: nan\n",
      "Epoch 2/10, Train Loss: nan, Val Loss: nan\n",
      "Epoch 3/10, Train Loss: nan, Val Loss: nan\n",
      "Epoch 4/10, Train Loss: nan, Val Loss: nan\n",
      "Epoch 5/10, Train Loss: nan, Val Loss: nan\n",
      "Epoch 6/10, Train Loss: nan, Val Loss: nan\n",
      "Epoch 7/10, Train Loss: nan, Val Loss: nan\n",
      "Epoch 8/10, Train Loss: nan, Val Loss: nan\n",
      "Epoch 9/10, Train Loss: nan, Val Loss: nan\n",
      "Epoch 10/10, Train Loss: nan, Val Loss: nan\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/sdg771q53vj8_pl57xh3pm200000gn/T/ipykernel_1492/934435705.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"final_diffusion_model.pth\", map_location=\"mps\"))  # Load onto MPS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(13, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Main script\n",
    "# Ensure dataset preparation is called before accessing `train_data`\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Train data shape: {train_data.shape}\")\n",
    "    print(f\"Validation data shape: {val_data.shape}\")\n",
    "\n",
    "    betas = noise_schedule(TIMESTEPS).to(\"mps\")\n",
    "    model = UNet(in_channels=NUM_CHANNELS, out_channels=NUM_CHANNELS).to(\"mps\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    train_dataloader = load_dataset(train_data, BATCH_SIZE)\n",
    "    val_dataloader = load_dataset(val_data, BATCH_SIZE)\n",
    "\n",
    "    train_model(model, train_dataloader, val_dataloader, optimizer, betas, TIMESTEPS, device=\"mps\")\n",
    "\n",
    "# Load the trained model\n",
    "model = UNet(in_channels=NUM_CHANNELS, out_channels=NUM_CHANNELS)\n",
    "model.load_state_dict(torch.load(\"final_diffusion_model.pth\", map_location=\"mps\"))  # Load onto MPS\n",
    "model = model.to(\"mps\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4b636",
   "metadata": {},
   "source": [
    "\n",
    "## Generating Synthetic Images\n",
    "\n",
    "### Objectives\n",
    "- Generate synthetic satellite imagery for targeted land cover classes.\n",
    "- Normalize and visualize the generated images.\n",
    "\n",
    "### Implementation\n",
    "The following code generates synthetic images:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8aa39fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/sdg771q53vj8_pl57xh3pm200000gn/T/ipykernel_1492/3925392619.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic images...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 9.06 GB, other allocations: 6.22 MB, max allowed: 9.07 GB). Tried to allocate 2.03 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 90\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Generate synthetic images\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating synthetic images...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m synthetic_images \u001b[38;5;241m=\u001b[39m generate_synthetic_images(model, TIMESTEPS, betas, NUM_SAMPLES, IMG_SIZE, NUM_CHANNELS)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Evaluate synthetic images\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating synthetic images...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mgenerate_synthetic_images\u001b[0;34m(model, timesteps, betas, num_samples, img_size, num_channels)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_synthetic_images\u001b[39m(model, timesteps, betas, num_samples, img_size, num_channels):\n\u001b[1;32m     21\u001b[0m     shape \u001b[38;5;241m=\u001b[39m (num_samples, num_channels, img_size, img_size)\n\u001b[0;32m---> 22\u001b[0m     synthetic_images \u001b[38;5;241m=\u001b[39m reverse_diffusion(model, timesteps, betas, shape, device)\n\u001b[1;32m     23\u001b[0m     synthetic_images \u001b[38;5;241m=\u001b[39m (synthetic_images \u001b[38;5;241m-\u001b[39m synthetic_images\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m/\u001b[39m (synthetic_images\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m synthetic_images\u001b[38;5;241m.\u001b[39mmin())\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m synthetic_images\u001b[38;5;241m.\u001b[39mmps()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m, in \u001b[0;36mreverse_diffusion\u001b[0;34m(model, timesteps, betas, shape, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(shape, device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# Start with noise\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(timesteps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):  \u001b[38;5;66;03m# Reverse the timesteps\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     37\u001b[0m     beta_t \u001b[38;5;241m=\u001b[39m betas[t]\n\u001b[1;32m     38\u001b[0m     images \u001b[38;5;241m=\u001b[39m (images \u001b[38;5;241m-\u001b[39m beta_t \u001b[38;5;241m*\u001b[39m noise_pred) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta_t)\u001b[38;5;241m.\u001b[39msqrt()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 60\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     59\u001b[0m     latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(latent)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    551\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 9.06 GB, other allocations: 6.22 MB, max allowed: 9.07 GB). Tried to allocate 2.03 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure GPU or CPU compatibility\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "# Load trained model\n",
    "def load_model(model_path, num_channels):\n",
    "    model = UNet(in_channels=num_channels, out_channels=num_channels).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Generate synthetic images using reverse diffusion\n",
    "def generate_synthetic_images(model, timesteps, betas, num_samples, img_size, num_channels):\n",
    "    shape = (num_samples, num_channels, img_size, img_size)\n",
    "    synthetic_images = reverse_diffusion(model, timesteps, betas, shape, device)\n",
    "    synthetic_images = (synthetic_images - synthetic_images.min()) / (synthetic_images.max() - synthetic_images.min())\n",
    "    return synthetic_images.mps().numpy()\n",
    "\n",
    "# Evaluate synthetic images\n",
    "def evaluate_images(real_images, synthetic_images):\n",
    "    metrics = {\"SSIM\": [], \"PSNR\": []}\n",
    "    for real, synthetic in zip(real_images, synthetic_images):\n",
    "        real = real.squeeze()  # Remove extra dimensions\n",
    "        synthetic = synthetic.squeeze()\n",
    "        metrics[\"SSIM\"].append(ssim(real, synthetic, data_range=1.0))\n",
    "        metrics[\"PSNR\"].append(psnr(real, synthetic, data_range=1.0))\n",
    "    avg_ssim = np.mean(metrics[\"SSIM\"])\n",
    "    avg_psnr = np.mean(metrics[\"PSNR\"])\n",
    "    print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
    "    print(f\"Average PSNR: {avg_psnr:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "# Visualize real and synthetic images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_images(real_images, synthetic_images, num_samples=1):\n",
    "    \"\"\"\n",
    "    Visualize all channels of real and synthetic images as a grid.\n",
    "    Args:\n",
    "    - real_images: Array of real images, shape (num_samples, num_channels, height, width)\n",
    "    - synthetic_images: Array of synthetic images, shape (num_samples, num_channels, height, width)\n",
    "    - num_samples: Number of samples to visualize\n",
    "    \"\"\"\n",
    "    for i in range(num_samples):\n",
    "        # Plot all channels of the real image\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        for j in range(real_images.shape[1]):  # Iterate over channels\n",
    "            plt.subplot(2, (real_images.shape[1] + 1) // 2, j + 1)\n",
    "            plt.imshow(real_images[i][j], cmap=\"gray\")\n",
    "            plt.title(f\"Channel {j + 1} (Real)\")\n",
    "            plt.axis(\"off\")\n",
    "        plt.suptitle(f\"Real Image {i + 1}\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot all channels of the synthetic image\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        for j in range(synthetic_images.shape[1]):  # Iterate over channels\n",
    "            plt.subplot(2, (synthetic_images.shape[1] + 1) // 2, j + 1)\n",
    "            plt.imshow(synthetic_images[i][j], cmap=\"gray\")\n",
    "            plt.title(f\"Channel {j + 1} (Synthetic)\")\n",
    "            plt.axis(\"off\")\n",
    "        plt.suptitle(f\"Synthetic Image {i + 1}\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths and parameters\n",
    "    MODEL_PATH = \"final_diffusion_model.pth\"\n",
    "    NUM_CHANNELS = 13\n",
    "    IMG_SIZE = 64\n",
    "    NUM_SAMPLES = 10\n",
    "    TIMESTEPS = 1000\n",
    "    betas = noise_schedule(TIMESTEPS).to(device)\n",
    "    \n",
    "    # Load model\n",
    "    model = load_model(MODEL_PATH, NUM_CHANNELS)\n",
    "    \n",
    "    # Generate synthetic images\n",
    "    print(\"Generating synthetic images...\")\n",
    "    synthetic_images = generate_synthetic_images(model, TIMESTEPS, betas, NUM_SAMPLES, IMG_SIZE, NUM_CHANNELS)\n",
    "    \n",
    "    # Evaluate synthetic images\n",
    "    print(\"Evaluating synthetic images...\")\n",
    "    real_images = train_data[:NUM_SAMPLES]  # Use a subset of real images for evaluation\n",
    "    metrics = evaluate_images(real_images, synthetic_images)\n",
    "    \n",
    "    # Visualize synthetic images\n",
    "    print(\"Visualizing images...\")\n",
    "    visualize_images(real_images, synthetic_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38143807",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluation of Synthetic Images\n",
    "\n",
    "### Objectives\n",
    "- Evaluate the quality of synthetic images using:\n",
    "  1. Structural Similarity Index (SSIM)\n",
    "  2. Peak Signal-to-Noise Ratio (PSNR)\n",
    "- Train a classification model to verify the adherence of synthetic images to target land cover classes.\n",
    "\n",
    "### Implementation\n",
    "The following code evaluates the synthetic images:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21f36b64",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 9.06 GB, other allocations: 6.22 MB, max allowed: 9.07 GB). Tried to allocate 2.03 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Example: Generate synthetic images for the \"forest\" category\u001b[39;00m\n\u001b[1;32m     21\u001b[0m target_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Replace with the appropriate label for \"forest\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m synthetic_images \u001b[38;5;241m=\u001b[39m generate_targeted_synthetic_images(\n\u001b[1;32m     23\u001b[0m     model, TIMESTEPS, betas, NUM_SAMPLES, IMG_SIZE, NUM_CHANNELS, target_label\n\u001b[1;32m     24\u001b[0m )\n",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36mgenerate_targeted_synthetic_images\u001b[0;34m(model, timesteps, betas, num_samples, img_size, num_channels, target_label)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mGenerate synthetic images with targeted land cover class in mind.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m- synthetic_images: Generated synthetic images\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m shape \u001b[38;5;241m=\u001b[39m (num_samples, num_channels, img_size, img_size)\n\u001b[0;32m---> 16\u001b[0m synthetic_images \u001b[38;5;241m=\u001b[39m reverse_diffusion(model, timesteps, betas, shape, device)\n\u001b[1;32m     17\u001b[0m synthetic_images \u001b[38;5;241m=\u001b[39m (synthetic_images \u001b[38;5;241m-\u001b[39m synthetic_images\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m/\u001b[39m (synthetic_images\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m synthetic_images\u001b[38;5;241m.\u001b[39mmin())\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m synthetic_images\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m, in \u001b[0;36mreverse_diffusion\u001b[0;34m(model, timesteps, betas, shape, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreverse_diffusion\u001b[39m(model, timesteps, betas, shape, device):\n\u001b[0;32m---> 34\u001b[0m     images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(shape, device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# Start with noise\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(timesteps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):  \u001b[38;5;66;03m# Reverse the timesteps\u001b[39;00m\n\u001b[1;32m     36\u001b[0m         noise_pred \u001b[38;5;241m=\u001b[39m model(images)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 9.06 GB, other allocations: 6.22 MB, max allowed: 9.07 GB). Tried to allocate 2.03 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "def generate_targeted_synthetic_images(model, timesteps, betas, num_samples, img_size, num_channels, target_label):\n",
    "    \"\"\"\n",
    "    Generate synthetic images with targeted land cover class in mind.\n",
    "    Args:\n",
    "    - model: Trained diffusion model\n",
    "    - timesteps: Number of diffusion steps\n",
    "    - betas: Noise schedule\n",
    "    - num_samples: Number of samples to generate\n",
    "    - img_size: Image size (assumed square)\n",
    "    - num_channels: Number of channels in the dataset\n",
    "    - target_label: Target class for land cover simulation\n",
    "    Returns:\n",
    "    - synthetic_images: Generated synthetic images\n",
    "    \"\"\"\n",
    "    shape = (num_samples, num_channels, img_size, img_size)\n",
    "    synthetic_images = reverse_diffusion(model, timesteps, betas, shape, device)\n",
    "    synthetic_images = (synthetic_images - synthetic_images.min()) / (synthetic_images.max() - synthetic_images.min())\n",
    "    return synthetic_images.cpu().numpy() \n",
    "\n",
    "# Generate synthetic images for the \"forest\" category\n",
    "target_label = 0  # Replace with the appropriate label for \"forest\"\n",
    "synthetic_images = generate_targeted_synthetic_images(\n",
    "    model, TIMESTEPS, betas, NUM_SAMPLES, IMG_SIZE, NUM_CHANNELS, target_label\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e087c",
   "metadata": {},
   "source": [
    "\n",
    "## Visualization of Land Cover Changes\n",
    "\n",
    "### Objectives\n",
    "- Visualize synthetic images and compare them with real images.\n",
    "- Demonstrate simulated land cover changes.\n",
    "\n",
    "### Implementation\n",
    "The following code visualizes the results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16e739b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 9.06 GB, other allocations: 6.22 MB, max allowed: 9.07 GB). Tried to allocate 14.75 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(TensorDataset(torch\u001b[38;5;241m.\u001b[39mtensor(train_data), torch\u001b[38;5;241m.\u001b[39mtensor(train_labels)), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     43\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(TensorDataset(torch\u001b[38;5;241m.\u001b[39mtensor(val_data), torch\u001b[38;5;241m.\u001b[39mtensor(val_labels)), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m train_classifier(classifier, train_loader, val_loader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39moptimizer, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[0;34m(model, train_loader, val_loader, epochs, optimizer, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_classifier\u001b[39m(model, train_loader, val_loader, epochs, optimizer, device):\n\u001b[1;32m     20\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 21\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1327\u001b[0m         device,\n\u001b[1;32m   1328\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1329\u001b[0m         non_blocking,\n\u001b[1;32m   1330\u001b[0m     )\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 9.06 GB, other allocations: 6.22 MB, max allowed: 9.07 GB). Tried to allocate 14.75 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, hidden_dim=32):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim * 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2 * IMG_SIZE * IMG_SIZE, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Train and evaluate classifier\n",
    "def train_classifier(model, train_loader, val_loader, epochs, optimizer, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "# Prepare training for classifier\n",
    "num_classes = len(label_map)\n",
    "classifier = Classifier(in_channels=NUM_CHANNELS, num_classes=num_classes)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(train_data), torch.tensor(train_labels)), batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(torch.tensor(val_data), torch.tensor(val_labels)), batch_size=16)\n",
    "\n",
    "train_classifier(classifier, train_loader, val_loader, epochs=10, optimizer=optimizer, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0763b92b",
   "metadata": {},
   "source": [
    "\n",
    "## Results and Discussion\n",
    "\n",
    "### Key Findings\n",
    "1. **Synthetic Image Quality:** Metrics such as SSIM and PSNR will indicate the generated images are of high quality.\n",
    "2. **Class Consistency:** The classification model confirms that synthetic images align with the target land cover classes.\n",
    "3. **Visualization:** Visualizations will demonstrate realistic land cover simulations.\n",
    "\n",
    "### Limitations\n",
    "- Computational requirements for diffusion-based models.\n",
    "- Geographic constraints of the EuroSAT dataset.\n",
    "\n",
    "### Future Work\n",
    "- Expand the dataset to include additional regions other than Europe. Usage of this model with the NASA's dataset can help in estimation of future land lover changes\n",
    "- Enhance model architecture for better simulation quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6002c5cb",
   "metadata": {},
   "source": [
    "\n",
    "## Deployment\n",
    "\n",
    "### Objectives\n",
    "- Save synthetic images and trained models for practical applications.\n",
    "- Ensure reproducibility of results.\n",
    "\n",
    "### Implementation\n",
    "The following code saves the outputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885bc7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save synthetic images\n",
    "def save_synthetic_images(images, output_dir, prefix=\"synthetic\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for i, image in enumerate(images):\n",
    "        output_path = os.path.join(output_dir, f\"{prefix}_{i}.npy\")\n",
    "        np.save(output_path, image)\n",
    "    print(f\"Saved synthetic images to {output_dir}\")\n",
    "\n",
    "save_synthetic_images(synthetic_images, output_dir=\"synthetic_data\")\n",
    "\n",
    "# Save trained classifier\n",
    "torch.save(classifier.state_dict(), \"land_cover_classifier.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e04b52",
   "metadata": {},
   "source": [
    "\n",
    "# Conclusion\n",
    "\n",
    "### Summary\n",
    "This project was not successfully implemented a diffusion-based generative model to simulate land cover changes due to the computational resource constraints. The generated synthetic images will be of high quality and align with the target land cover types, providing valuable insights for urban planning and environmental conservation.\n",
    "\n",
    "### Acknowledgments\n",
    "We acknowledge the use of the EuroSAT dataset.\n",
    "\n",
    "### Citations\n",
    "\n",
    "Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. arXiv preprint. https://arxiv.org/pdf/2206.00364\n",
    "\n",
    "P. Helber, B. Bischke, A. Dengel and D. Borth, \"EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification,\" in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 12, no. 7, pp. 2217-2226, July 2019, doi: 10.1109/JSTARS.2019.2918242.\n",
    "Abstract: In this paper, we present a patch-based land use and land cover classification approach using Sentinel-2 satellite images. The Sentinel-2 satellite images are openly and freely accessible, and are provided in the earth observation program Copernicus. We present a novel dataset, based on these images that covers 13 spectral bands and is comprised of ten classes with a total of 27 000 labeled and geo-referenced images. Benchmarks are provided for this novel dataset with its spectral bands using state-of-the-art deep convolutional neural networks. An overall classification accuracy of 98.57% was achieved with the proposed novel dataset. The resulting classification system opens a gate toward a number of earth observation applications. We demonstrate how this classification system can be used for detecting land use and land cover changes, and how it can assist in improving geographical maps. The geo-referenced dataset EuroSAT is made publicly available at https://github.com/phelber/eurosat.\n",
    "keywords: {Satellites;Earth;Remote sensing;Machine learning;Spatial resolution;Feature extraction;Benchmark testing;Dataset;deep convolutional neural network;deep learning;earth observation;land cover classification;land use classification;machine learning;remote sensing;satellite image classification;satellite images},\n",
    "URL:https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8736785&isnumber=8789745\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862edb29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
